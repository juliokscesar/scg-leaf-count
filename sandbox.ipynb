{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd0007e-5619-4437-8582-945e85511c70",
   "metadata": {},
   "source": [
    "# Color Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70380ce-d5bb-4856-814e-bcf396e78ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import analyze_color_histogram\n",
    "from scg_detection_tools.models import YOLO_NAS, YOLOv8\n",
    "from scg_detection_tools.detect import Detector\n",
    "from scg_detection_tools.utils.file_handling import read_yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import cv2\n",
    "\n",
    "cfg = read_yaml(\"analyze_config.yaml\")\n",
    "model = YOLOv8(yolov8_ckpt_path=cfg[\"yolov8_model_path\"])\n",
    "#model = YOLO_NAS(model_arch=cfg[\"yolonas_arch\"], \n",
    "#                checkpoint_path=cfg[\"yolonas_model_path\"], \n",
    "#                classes=cfg[\"data_classes\"])\n",
    "det_params = cfg[\"detect_parameters\"]\n",
    "det_params[\"embed_slice_callback\"] = None\n",
    "det = Detector(detection_model=model, detection_params=det_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd930a-3420-4bd3-afa2-2dbf70c74a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scg_detection_tools.utils.file_handling import get_all_files_from_paths\n",
    "\n",
    "IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/images\"\n",
    "LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/labels\"\n",
    "\n",
    "#IMG_DIR = \"/home/julio/Dev/SCG_IFSC/save/hemacias/images\"\n",
    "\n",
    "imgs = get_all_files_from_paths(IMG_DIR)\n",
    "\n",
    "img_hists = analyze_color_histogram(model=model, \n",
    "                                    detector=det, \n",
    "                                    imgs=imgs, \n",
    "                                    raw=False, \n",
    "                                    on_detection_boxes=False,\n",
    "                                    seg_annotations=LBL_DIR, \n",
    "                                    cspaces=[\"RGB\", \"HSV\", \"GRAY\"], \n",
    "                                    show=True, save_plots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f61d7-497f-4a46-9982-1e269442e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single channel images\n",
    "\n",
    "import os\n",
    "\n",
    "save = False\n",
    "for img in imgs:\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12,8))\n",
    "    \n",
    "    rgb = cv2.imread(img)\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    hsv = cv2.cvtColor(rgb.copy(), cv2.COLOR_RGB2HSV)\n",
    "    LABELS = [\"RGB\", \"HSV\"]\n",
    "    for ci, cp in enumerate([rgb, hsv]):\n",
    "        for i in range(3):\n",
    "            ch = cp.copy()\n",
    "            for j in range(3):\n",
    "                if i != j:\n",
    "                    ch[:,:,j] = 0\n",
    "            axs[ci][i].axis(\"off\")\n",
    "            axs[ci][i].imshow(ch)\n",
    "            axs[ci][i].set_title(LABELS[ci][i])\n",
    "    if save:\n",
    "        fig.savefig(f\"exp_analysis/singlechannel_{os.path.basename(img)}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bb6cf-1256-4cd2-b132-47aee92ce337",
   "metadata": {},
   "outputs": [],
   "source": [
    "cspaceAnalysis = {\"RGB\": [], \"HSV\": [], \"GRAY\": [], \"ALL\": []}\n",
    "nclass = {0: \"light\", 1: \"medium\", 2: \"dark\", 3: \"dead\"}\n",
    "intensities = np.arange(256)\n",
    "for img in img_hists:\n",
    "    img_hist = img_hists[img]\n",
    "    for cspace in [\"RGB\", \"HSV\", \"GRAY\"]:\n",
    "        for mask_hist in img_hist[cspace][\"masks\"]:\n",
    "            hist = mask_hist[\"hist\"]\n",
    "            mask_class = nclass[mask_hist[\"class\"]]\n",
    "\n",
    "            ch_stats = []\n",
    "            for ch_hist in hist:\n",
    "                ch_mean = np.sum(intensities * ch_hist) / np.sum(ch_hist)\n",
    "                ch_std = np.sqrt(np.sum((intensities - ch_mean) ** 2 * ch_hist) / np.sum(ch_hist))\n",
    "                ch_stats.append([ch_mean, ch_std])\n",
    "            \n",
    "            ch_stats = np.array(ch_stats).T.ravel().tolist()\n",
    "            ch_stats.append(mask_class)\n",
    "            cspaceAnalysis[cspace].append(ch_stats)\n",
    "\n",
    "\n",
    "num_masks = len(cspaceAnalysis[\"RGB\"])\n",
    "all_mean = []\n",
    "all_std = []\n",
    "all_class = [data[-1] for data in cspaceAnalysis[\"RGB\"]]\n",
    "\n",
    "for maskidx in range(num_masks):\n",
    "    all_mean.append([])\n",
    "    all_std.append([])\n",
    "    for cspace in [\"RGB\", \"HSV\", \"GRAY\"]:\n",
    "        data = cspaceAnalysis[cspace][maskidx]\n",
    "        if cspace == \"GRAY\":\n",
    "            ch_mean = data[0]\n",
    "            ch_std = data[1]\n",
    "            all_mean[-1].append(ch_mean)\n",
    "            all_std[-1].append(ch_std)\n",
    "        else:\n",
    "            ch_mean = data[:3]\n",
    "            ch_std = data[3:-1]\n",
    "            all_mean[-1].extend(ch_mean)\n",
    "            all_std[-1].extend(ch_std)\n",
    "for i in range(num_masks):\n",
    "    cspaceAnalysis[\"ALL\"].append([])\n",
    "    \n",
    "    cspaceAnalysis[\"ALL\"][i].extend(all_mean[i])\n",
    "    cspaceAnalysis[\"ALL\"][i].extend(all_std[i])\n",
    "    cspaceAnalysis[\"ALL\"][i].append(all_class[i])\n",
    "\n",
    "rgbdf = pd.DataFrame(cspaceAnalysis[\"RGB\"], columns=[\n",
    "    \"R Mean\", \"G Mean\", \"B Mean\", \"R std\", \"G std\", \"B std\", \"Class\"\n",
    "])\n",
    "hsvdf = pd.DataFrame(cspaceAnalysis[\"HSV\"], columns=[\n",
    "    \"H Mean\", \"S Mean\", \"V Mean\", \"H std\", \"S std\", \"V std\", \"Class\"\n",
    "])\n",
    "graydf = pd.DataFrame(cspaceAnalysis[\"GRAY\"], columns=[\n",
    "    \"Gray mean\", \"Gray std\", \"Class\"\n",
    "])\n",
    "\n",
    "alldf = pd.DataFrame(cspaceAnalysis[\"ALL\"], columns=[\n",
    "    \"R Mean\", \"G Mean\", \"B Mean\", \"H Mean\", \"S Mean\", \"V Mean\", \"Gray mean\", \"R std\", \"G std\", \"B std\", \"H std\", \"S std\", \"V std\", \"Gray std\", \"Class\"\n",
    "])\n",
    "\n",
    "class_c_map = {\n",
    "    \"light\": \"blue\", \n",
    "    \"medium\": \"green\", \n",
    "    \"dark\": \"red\", \n",
    "    \"dead\": \"black\"\n",
    "}\n",
    "\n",
    "class_color = lambda df: df[\"Class\"].map(class_c_map)\n",
    "\n",
    "scatter_matrix(rgbdf.iloc[:,:-1], figsize=(12,12), diagonal=\"kde\", color=class_color(rgbdf))\n",
    "scatter_matrix(hsvdf.iloc[:,:-1], figsize=(12,12), diagonal=\"kde\", color=class_color(hsvdf))\n",
    "scatter_matrix(graydf.iloc[:,:-1], figsize=(12,12), diagonal=\"kde\", color=class_color(graydf))\n",
    "scatter_matrix(alldf.iloc[:,:-1], figsize=(20,20), diagonal=\"kde\", color=class_color(alldf))\n",
    "\n",
    "print(rgbdf.corr(numeric_only=True))\n",
    "print(hsvdf.corr(numeric_only=True))\n",
    "print(graydf.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767042a3-1664-48f2-b52c-df81867a4bf0",
   "metadata": {},
   "source": [
    "# Clustering and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998ca563-9aad-49f7-bd4a-a7078d69845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KMeans:\n",
    "\n",
    "Use k-means to find clusters based on leaf masks extracted with yolo and sam2\n",
    "- (for now) use the features from above (RGB, HSV, Gray means and std), pass them to PSA\n",
    "- to get it to 2 and 3 dimensions (compare them) then use them to train KMeans\n",
    "- and also plot inertia x n_clusters to check for optimal values\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# alldf is a dataset of 14 features (RGB,HSV,Gray)(Mean,STD) not including class\n",
    "# it will only be used to color markers in plot\n",
    "#print(alldf.head())\n",
    "#print(alldf.iloc[:,:-1].values)\n",
    "X = alldf.iloc[:,:-1].values\n",
    "\n",
    "# Use PCA to reduce to 2d\n",
    "reduced = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# Feature scaling to normalize values and have a stdv of 1 and mean of 0\n",
    "# so first scale then apply PCA\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_PCA = PCA(n_components=2).fit_transform(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208b849-a7be-42d4-8476-d69fa5903fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze inertia to choose best numbmer for clusters\n",
    "inertia = []\n",
    "MAX_CLUSTERS = 15\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,8), layout=\"tight\")\n",
    "for i, (name, data) in enumerate([(\"PCA reduced\",reduced), (\"Scale + PCA reduced\",scaled_PCA)]):\n",
    "    inertia = []\n",
    "    for n in range(1, MAX_CLUSTERS):\n",
    "        kmeans = KMeans(n_clusters=n, init=\"k-means++\", n_init=10, max_iter=300,\n",
    "                        tol=0.0001, random_state=0, algorithm=\"lloyd\")\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    axs[i].plot(np.arange(1, MAX_CLUSTERS), inertia, marker='o')\n",
    "    axs[i].set(xlabel=\"# Clusters\", ylabel=\"Inertia\", title=name)\n",
    "    del inertia\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886ba00-3dae-4b47-9dda-8228a1326958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_test(n_clusters, data, name):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", max_iter=300,\n",
    "                   tol=0.001, random_state=0, algorithm=\"lloyd\")\n",
    "\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    ## visualization\n",
    "    step = 0.02\n",
    "    x_min, x_max = data[:,0].min() - 1, data[:,0].max() + 1\n",
    "    y_min, y_max = data[:,1].min() - 1, data[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(Z, interpolation=\"nearest\", extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "          cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "    for c in class_c_map:\n",
    "        c_data = data[alldf[\"Class\"] == c,:]\n",
    "        ax.scatter(c_data[:,0], c_data[:,1], color=class_c_map[c], alpha=0.8)\n",
    "    \n",
    "    #ax.plot(reduced[:,0], reduced[:,1], )\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    ax.scatter(centroids[:,0], centroids[:,1], marker=\"x\", s=169, color=\"w\", zorder=10, linewidth=3)\n",
    "    ax.set(title=f\"K-Means clustering on leaves masks using {name}\",\n",
    "           xlim=(x_min, x_max),\n",
    "           ylim=(y_min, y_max))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b99d3-d2ba-414e-aeb1-93245849b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_test(n_clusters=4, data=reduced, name=\"PCA reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a38d6-1ac3-4a6e-8391-7afc6a1e9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_test(n_clusters=4, data=scaled_PCA, name=\"Scale + PCA reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0b15e-d5ff-45f5-8c36-4c9f5c5c1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with sklearn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "N_CLUSTERS = 4\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "pca = PCA(n_components=2)\n",
    "pipe = Pipeline(\n",
    "    steps=[(\"scaler\", scaler), (\"pca\", pca)]\n",
    ")\n",
    "transformed = pipe.fit_transform(X)\n",
    "kmeans_test(n_clusters=N_CLUSTERS, data=transformed, name=\"Pipeline: Scaler to PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2533cb3f-2082-4157-bcb4-a231c83c9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K Nearest Neighbors:\n",
    "- To decide on the value of K, test for a range\n",
    "- Use a pipeline\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = alldf.iloc[:,:-1].values\n",
    "y = alldf.iloc[:,-1].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded, stratify=encoded, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=5)),\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len X_test: {len(X_test)}\")\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(f\"predicted,real:\\n{[(p, r) for p,r in zip(encoder.inverse_transform(predicted), encoder.inverse_transform(y_test))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caade77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd19db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = np.arange(1, 15+1)\n",
    "weights = (\"uniform\", \"distance\")\n",
    "\n",
    "for n in n_neighbors:\n",
    "    for weight in weights:\n",
    "        clf = Pipeline(steps=[\n",
    "          (\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=n, weights=weight)),\n",
    "        ])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"_\"*82)\n",
    "\n",
    "        print(f\"Classification report for k={n}, weights={weight!r}\")\n",
    "        print(classification_report(y_test, clf.predict(X_test), target_names=encoder.classes_))\n",
    "\n",
    "        print(\"_\"*82)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d463469",
   "metadata": {},
   "source": [
    "# Leaf color identification pipeline (light, medium, dark, dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2f0de",
   "metadata": {},
   "source": [
    "### Using KNN trained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get detection box\n",
    "# segment leaf to exclude background (maybe not even needed)\n",
    "# pass class + pixels to data\n",
    "\n",
    "# -> specify size to standardize input (like 50x50, 80x80)\n",
    "# -> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8169b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scg_detection_tools.models import YOLO_NAS, YOLOv8\n",
    "from scg_detection_tools.detect import Detector\n",
    "from scg_detection_tools.utils.file_handling import read_yaml\n",
    "import scg_detection_tools.utils.image_tools as imtools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import cv2\n",
    "\n",
    "cfg = read_yaml(\"analyze_config.yaml\")\n",
    "# model = YOLO_NAS(model_arch=cfg[\"yolonas_arch\"], \n",
    "#                 checkpoint_path=cfg[\"yolonas_model_path\"], \n",
    "#                 classes=cfg[\"data_classes\"])\n",
    "model = YOLOv8(yolov8_ckpt_path=cfg[\"yolov8_model_path\"])\n",
    "det_params = cfg[\"detect_parameters\"]\n",
    "det_params[\"embed_slice_callback\"] = None\n",
    "det = Detector(detection_model=model, detection_params=det_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea8e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scg_detection_tools.utils.image_tools as imtools\n",
    "import scg_detection_tools.utils.cvt as cvt\n",
    "from scg_detection_tools.utils.file_handling import get_all_files_from_paths\n",
    "from scg_detection_tools.dataset import read_dataset_annotation\n",
    "from analyze import parse_seg_annotations\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/images\"\n",
    "LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/labels\"\n",
    "\n",
    "imgs = get_all_files_from_paths(IMG_DIR, skip_ext=[\".txt\", \".json\", \".yaml\"])\n",
    "ann_files, img_ann_idx = parse_seg_annotations(imgs, LBL_DIR)\n",
    "\n",
    "# Keep track of every object as (nclass, obj_crop)\n",
    "obj_data = []\n",
    "\n",
    "# CHOOSING 32x32 because of calculated average\n",
    "STANDARD_SIZE = (32, 32)\n",
    "\n",
    "# !!!!!! taken from data.yaml\n",
    "class_map = {0: \"dark\", 1: \"dead\", 2: \"light\", 3: \"medium\"}\n",
    "\n",
    "for img in imgs:\n",
    "    ann_file = ann_files[img_ann_idx[img]]\n",
    "    annotations = read_dataset_annotation(ann_file, separate_class=False)\n",
    "\n",
    "    # check if contours are boxes or segments\n",
    "    orig = cv2.imread(img)\n",
    "    orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "    imgsz = orig.shape[:2]\n",
    "\n",
    "    for ann in annotations:\n",
    "        nclass = ann[0]\n",
    "        contour = ann[1:]\n",
    "        if len(contour) == 4:\n",
    "            mask = cvt.boxes_to_masks([contour], imgsz=imgsz, normalized=True)[0]\n",
    "        else:\n",
    "            mask = cvt.contours_to_masks([contour], imgsz=imgsz, normalized=True)[0]\n",
    "        \n",
    "        # get only segmented object from image\n",
    "        masked = deepcopy(orig)\n",
    "        masked[mask[:,:] < 255] = 0\n",
    "\n",
    "        # crop a box around it\n",
    "        points = np.array(contour).reshape(len(contour) // 2, 2)\n",
    "        box = cvt.segment_to_box(points, normalized=True, imgsz=imgsz)\n",
    "        obj_crop = imtools.crop_box_image(masked, box)\n",
    "\n",
    "        # resize to 32x32 and add to our data\n",
    "        obj_crop = cv2.resize(obj_crop, STANDARD_SIZE, cv2.INTER_CUBIC)\n",
    "        obj_data.append((nclass, obj_crop))\n",
    "\n",
    "\n",
    "# Get average object size\n",
    "# avg_h = 0\n",
    "# avg_w = 0\n",
    "# for obj in obj_data:\n",
    "#     h, w = obj[1].shape[:2]\n",
    "#     avg_h += h\n",
    "#     avg_w += w\n",
    "# print(f\"Average: W={avg_w / len(obj_data)}, H={avg_h / len(obj_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 'obj_data' and split it to have a 'X' vector including the object crops and 'y' being the classes\n",
    "y = []\n",
    "X = []\n",
    "MAX_MEDIUM = 150\n",
    "inc_medium = 0\n",
    "for (nclass, obj_crop) in obj_data:\n",
    "    if nclass == 3:\n",
    "        if inc_medium >= 150:\n",
    "            continue\n",
    "        inc_medium += 1\n",
    "\n",
    "    y.append(nclass)\n",
    "\n",
    "    # For the X vector, we want to include the object as a 32x32x3=3072 length vector\n",
    "    # but we're also including the HSV and Gray cropped object\n",
    "    # so the final array to append to our vector must be of size 32x32x(3 + 3 + 1) = 7168\n",
    "    hsv = cv2.cvtColor(obj_crop, cv2.COLOR_RGB2HSV)\n",
    "    gray = cv2.cvtColor(obj_crop, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    flat_rgb = obj_crop.flatten() # shape 32x32x3 = 3072\n",
    "    flat_hsv = hsv.flatten() # shape 32x32x3 = 3072\n",
    "    flat_gray = gray.flatten() # shape 32x32x1 = 1024\n",
    "\n",
    "    final = np.concatenate((flat_rgb, flat_hsv, flat_gray))\n",
    "    X.append(final)\n",
    "\n",
    "for c in class_map:\n",
    "    print(f\"Samples of class {class_map[c]!r} ({c}): {len([ i for i in y if i == c ])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95b4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify code test\n",
    "\n",
    "from analysis.classify import KNNClassifier\n",
    "\n",
    "knn = KNNClassifier(n_neighbors=5, weights=\"distance\")\n",
    "knn.train(X_train, y_train)\n",
    "knn.evaluate(X_test, y_test)\n",
    "knn.save_state(\"knn_k5_last.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train KNN with our data\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# ELBOW TEST TO CHOOSE N_NEIGHBORS\n",
    "# BEST RESULT SO FAR: K=5\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "for neigh in range(1, 21):\n",
    "    clf = Pipeline(\n",
    "        steps=[ \n",
    "            (\"scaler\", StandardScaler()), \n",
    "            #(\"nca\", NeighborhoodComponentsAnalysis(random_state=42)),\n",
    "            (\"knn\", KNeighborsClassifier(n_neighbors=neigh, weights=\"distance\")),\n",
    "        ]\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Test predictions\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(f\"CONFUSION MATRIX WITH K={neigh}\")\n",
    "    cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_map[c] for c in class_map])\n",
    "    print(class_map)\n",
    "    print(classification_report(y_test, predictions, labels=clf.classes_))\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization KNN\n",
    "    # pca = PCA(n_components=2)\n",
    "    # X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(12,10))\n",
    "    # for cls in np.unique(predictions):\n",
    "    #     ax.scatter(X_test_pca[predictions == cls, 0], \n",
    "    #             X_test_pca[predictions == cls, 1], \n",
    "    #             label=f\"{class_map[cls]}\", \n",
    "    #             s=50, \n",
    "    #             edgecolor='k', \n",
    "    #             alpha=0.8)\n",
    "    # ax.set(xlabel=\"PCA component 1\", ylabel=\"PCA component 2\")\n",
    "    # ax.legend(title=\"Predicted classes\")\n",
    "    # ax.set_title(f\"KNN with k={neigh}\")\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN with K=5\n",
    "N_NEIGHBORS = 5\n",
    "clf = Pipeline(\n",
    "    steps=[ \n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=N_NEIGHBORS, weights=\"distance\")),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "predictions = clf.predict(X_test)\n",
    "print(f\"CONFUSION MATRIX WITH K={neigh}\")\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_map[c] for c in class_map])\n",
    "print(class_map)\n",
    "print(classification_report(y_test, predictions, labels=clf.classes_))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ddd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SVM\n",
    "from sklearn import svm\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# TESTING DIFFERENT SVM KERNELS\n",
    "# BEST SO FAR: 'rbf'\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "for kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]:\n",
    "    sv_clf = Pipeline(\n",
    "        steps=[\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", svm.SVC(gamma=\"auto\", kernel=kernel))\n",
    "        ]\n",
    "    )\n",
    "    sv_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Test predictions\n",
    "    predictions = sv_clf.predict(X_test)\n",
    "    print(f\"CONFUSION MATRIX WITH KERNEL={kernel!r}\")\n",
    "    cm = confusion_matrix(y_test, predictions, labels=sv_clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_map[c] for c in class_map])\n",
    "    print(class_map)\n",
    "    print(classification_report(y_test, predictions, labels=sv_clf.classes_))\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with best result kernel: \"rbf\"\n",
    "kernel = \"rbf\"\n",
    "sv_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", svm.SVC(gamma=\"auto\", kernel=kernel))\n",
    "    ]\n",
    ")\n",
    "sv_clf.fit(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "predictions = sv_clf.predict(X_test)\n",
    "print(f\"CONFUSION MATRIX WITH KERNEL={kernel!r}\")\n",
    "cm = confusion_matrix(y_test, predictions, labels=sv_clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[class_map[c] for c in class_map])\n",
    "print(class_map)\n",
    "print(classification_report(y_test, predictions, labels=sv_clf.classes_))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.classify import SVMClassifier\n",
    "\n",
    "s = SVMClassifier(kernel=\"rbf\")\n",
    "s.train(X_train, y_train)\n",
    "s.evaluate(X_test, y_test)\n",
    "s.save_state(\"svm_rbf_last.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Using KNN trained above to predict on untrained image\n",
    "img = \"/home/juliocesar/leaf-detection/scg-leaf-count/imgs/test/5_640x640_aglom.png\"\n",
    "\n",
    "# -> get image detections\n",
    "# -> crop and segment\n",
    "# -> predict color class\n",
    "# -> display image with color labels\n",
    "\n",
    "detections = det(img)[0]\n",
    "ann_img = imtools.box_annotated_image(img, detections, box_thickness=2)\n",
    "imtools.plot_image(ann_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scg_detection_tools.segment import SAM2Segment\n",
    "\n",
    "seg = SAM2Segment(sam2_ckpt_path=cfg[\"sam2_ckpt_path\"],\n",
    "                  sam2_cfg=cfg[\"sam2_cfg\"])\n",
    "\n",
    "masks = seg._segment_detection(img, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c7ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scg_detection_tools.utils.cvt as cvt\n",
    "\n",
    "boxes = detections.xyxy.astype(np.int32)\n",
    "orig_img = cv2.imread(img)\n",
    "orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "obj_data = []\n",
    "OBJ_STD_SIZE = (32,32)\n",
    "\n",
    "# Apply mask to segment our image and crop a box around the segmented object\n",
    "for box, mask in zip(boxes, masks):\n",
    "    h, w = mask.shape[1:]\n",
    "    mask = mask.astype(np.uint8).reshape(h, w,)\n",
    "    mask = np.where(mask == 1, 255, 0)\n",
    "\n",
    "    masked = orig_img.copy()\n",
    "    masked[mask[:,:] < 255] = 0\n",
    "\n",
    "    obj_crop = imtools.crop_box_image(masked, box)\n",
    "    obj_crop = cv2.resize(obj_crop, OBJ_STD_SIZE, cv2.INTER_CUBIC)\n",
    "\n",
    "    obj_data.append([box, mask, obj_crop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process our object crops to prepare them as input for KNN\n",
    "# -> calculate hsv and gray, then make predictions and append class to our obj data\n",
    "for i, (box, mask, obj) in enumerate(obj_data):\n",
    "    hsv = cv2.cvtColor(obj, cv2.COLOR_RGB2HSV)\n",
    "    gray = cv2.cvtColor(obj, cv2.COLOR_RGB2GRAY)\n",
    "    rgb = obj\n",
    "\n",
    "    attributes = np.concatenate((rgb.flatten(), hsv.flatten(), gray.flatten()))\n",
    "    nclass = knn.predict([attributes])[0]\n",
    "    obj_data[i].append(nclass)\n",
    "\n",
    "print(len(obj_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b545c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from scg_detection_tools.utils.image_tools import segment_annotated_image\n",
    "\n",
    "# !!!!!! taken from data.yaml\n",
    "class_map = {0: \"dark\", 1: \"dead\", 2: \"light\", 3: \"medium\"}\n",
    "\n",
    "class_color = {\n",
    "    \"light\": \"deepskyblue\",\n",
    "    \"medium\": \"lime\",\n",
    "    \"dark\": \"red\",\n",
    "    \"dead\": \"black\",\n",
    "}\n",
    "\n",
    "color_patches = [\n",
    "    mpatches.Patch(color=class_color[c], label=c) for c in class_color \n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15,10))\n",
    "axs[0].imshow(orig_img)\n",
    "axs[1].imshow(orig_img)\n",
    "\n",
    "ann_img = orig_img.copy()\n",
    "for box, mask, obj_crop, nc in obj_data:\n",
    "\n",
    "    class_name = class_map[nc]\n",
    "    h, w = mask.shape\n",
    "    \n",
    "    color = []\n",
    "    color.extend(mcolors.to_rgb(class_color[class_name]))\n",
    "\n",
    "    ann_img = segment_annotated_image(ann_img, mask, color, alpha=0.6)\n",
    "\n",
    "axs[1].imshow(ann_img)\n",
    "axs[1].legend(handles=color_patches)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94204dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/juliocesar/leaf-detection/scg-leaf-count/.temp/2347c23ba8c7a229f034f714235448bedc3c5a3a97a29cc3.png: 640x640 122 leafs, 22.1ms\n",
      "Speed: 1.5ms preprocess, 22.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /home/juliocesar/leaf-detection/scg-leaf-count/.temp/7e0118fcb329a3f248d5f3d46eb7c89bb58189149ea2bcdf.png: 640x640 120 leafs, 22.0ms\n",
      "Speed: 1.0ms preprocess, 22.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /home/juliocesar/leaf-detection/scg-leaf-count/.temp/83cdd38a13df8aa75cf62f721e2524e6af18f735a4b2e774.png: 640x640 16 leafs, 21.5ms\n",
      "Speed: 1.0ms preprocess, 21.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-10 17:17:50] INFO - sam2_image_predictor.py - For numpy array image, we assume (HxWxC) format\n",
      "[2024-09-10 17:17:50] INFO - sam2_image_predictor.py - Computing image embeddings for the provided image...\n",
      "[2024-09-10 17:17:50] INFO - sam2_image_predictor.py - Image embeddings computed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     21\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/juliocesar/leaf-detection/scg-leaf-count/imgs/test/5_640x640_aglom.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/juliocesar/leaf-detection/scg-leaf-count/imgs/test/0_640x640.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     26\u001b[0m class_color \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepskyblue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdark\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdead\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m }\n\u001b[0;32m---> 33\u001b[0m \u001b[43manalyze_classify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcls_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcls_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msvm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msam2_ckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msam2_ckpt_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msam2_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msam2_cfg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/leaf-detection/scg-leaf-count/analyze.py:431\u001b[0m, in \u001b[0;36manalyze_classify\u001b[0;34m(detector, imgs, cls_labels, cls_colors, method, seg_annotations, sam2_ckpt_path, sam2_cfg, save)\u001b[0m\n\u001b[1;32m    427\u001b[0m         boxes\u001b[38;5;241m.\u001b[39mappend(box)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m seg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    432\u001b[0m     masks \u001b[38;5;241m=\u001b[39m seg\u001b[38;5;241m.\u001b[39m_segment_detection(img, detections)\n\u001b[1;32m    433\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m detections\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/detect.py:29\u001b[0m, in \u001b[0;36mDetector.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/detect.py:39\u001b[0m, in \u001b[0;36mDetector.detect_objects\u001b[0;34m(self, img, **diff_det_params)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_det_params[param] \u001b[38;5;241m=\u001b[39m diff_det_params[param]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_multiple_images(img)\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/detect.py:48\u001b[0m, in \u001b[0;36mDetector._detect_single_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m use_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_det_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_slice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_slice:\n\u001b[0;32m---> 48\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfidence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverlap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mslice_wh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_wh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mslice_overlap_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_overlap_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mslice_iou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_iou_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43membed_slice_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_det_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membed_slice_callback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_det_model\u001b[38;5;241m.\u001b[39mpredict(img_path\u001b[38;5;241m=\u001b[39mimage_path,\n\u001b[1;32m     57\u001b[0m                                          confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_det_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     58\u001b[0m                                          overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_det_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverlap\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/models.py:116\u001b[0m, in \u001b[0;36mYOLOv8.slice_predict\u001b[0;34m(self, img_path, confidence, overlap, slice_wh, slice_overlap_ratio, slice_iou_threshold, embed_slice_callback)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    109\u001b[0m                   img_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    110\u001b[0m                   confidence: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m                   slice_iou_threshold: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    115\u001b[0m                   embed_slice_callback: Callable[[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray], \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_wh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_overlap_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_iou_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_slice_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/models.py:76\u001b[0m, in \u001b[0;36mBaseDetectionModel.slice_predict\u001b[0;34m(self, img_path, confidence, overlap, slice_wh, slice_overlap_ratio, slice_iou_threshold, embed_slice_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     72\u001b[0m slicer \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mInferenceSlicer(callback\u001b[38;5;241m=\u001b[39msv_slice_callback,\n\u001b[1;32m     73\u001b[0m                             slice_wh\u001b[38;5;241m=\u001b[39mslice_wh,\n\u001b[1;32m     74\u001b[0m                             overlap_ratio_wh\u001b[38;5;241m=\u001b[39mslice_overlap_ratio,\n\u001b[1;32m     75\u001b[0m                             iou_threshold\u001b[38;5;241m=\u001b[39mslice_iou_threshold)\n\u001b[0;32m---> 76\u001b[0m sliced_detections \u001b[38;5;241m=\u001b[39m \u001b[43mslicer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sliced_detections\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/supervision/detection/tools/inference_slicer.py:139\u001b[0m, in \u001b[0;36mInferenceSlicer.__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    135\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    136\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_callback, image, offset) \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m offsets\n\u001b[1;32m    137\u001b[0m     ]\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[0;32m--> 139\u001b[0m         detections_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    141\u001b[0m merged \u001b[38;5;241m=\u001b[39m Detections\u001b[38;5;241m.\u001b[39mmerge(detections_list\u001b[38;5;241m=\u001b[39mdetections_list)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverlap_filter_strategy \u001b[38;5;241m==\u001b[39m OverlapFilter\u001b[38;5;241m.\u001b[39mNONE:\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/supervision/detection/tools/inference_slicer.py:168\u001b[0m, in \u001b[0;36mInferenceSlicer._run_callback\u001b[0;34m(self, image, offset)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mRun the provided callback on a slice of an image.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Detections: A collection of detections for the slice.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m image_slice \u001b[38;5;241m=\u001b[39m crop_image(image\u001b[38;5;241m=\u001b[39mimage, xyxy\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m--> 168\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_slice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m resolution_wh \u001b[38;5;241m=\u001b[39m (image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m detections \u001b[38;5;241m=\u001b[39m move_detections(\n\u001b[1;32m    171\u001b[0m     detections\u001b[38;5;241m=\u001b[39mdetections, offset\u001b[38;5;241m=\u001b[39moffset[:\u001b[38;5;241m2\u001b[39m], resolution_wh\u001b[38;5;241m=\u001b[39mresolution_wh\n\u001b[1;32m    172\u001b[0m )\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/models.py:60\u001b[0m, in \u001b[0;36mBaseDetectionModel.slice_predict.<locals>.sv_slice_callback\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmpfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     58\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(f\u001b[38;5;241m.\u001b[39mname, cv2\u001b[38;5;241m.\u001b[39mcvtColor(sliceimg, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[0;32m---> 60\u001b[0m     det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_slice_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         embed_slice_callback(img_path, sliceimg, tmpfile, det\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32))\n",
      "File \u001b[0;32m~/leaf-detection/scg-detection-tools/src/scg_detection_tools/models.py:99\u001b[0m, in \u001b[0;36mYOLOv8.predict\u001b[0;34m(self, img_path, confidence, overlap)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_path: \u001b[38;5;28mstr\u001b[39m, confidence: \u001b[38;5;28mfloat\u001b[39m, overlap: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections:\n\u001b[0;32m---> 99\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_underlying_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections\u001b[38;5;241m.\u001b[39mfrom_ultralytics(results[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m detections\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/engine/model.py:567\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/engine/predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:456\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 456\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/nn/tasks.py:106\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/nn/tasks.py:124\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/nn/tasks.py:145\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 145\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    146\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:915\u001b[0m, in \u001b[0;36mPSA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03mForward pass of the PSA module.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m a, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 915\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(b)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat((a, b), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leaf-detection/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:869\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    864\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\n\u001b[1;32m    865\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, N)\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m    866\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    867\u001b[0m )\n\u001b[0;32m--> 869\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    870\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    871\u001b[0m x \u001b[38;5;241m=\u001b[39m (v \u001b[38;5;241m@\u001b[39m attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(B, C, H, W) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(v\u001b[38;5;241m.\u001b[39mreshape(B, C, H, W))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "### ANALYSIS CLASSIFICATION TEST\n",
    "#########################################################\n",
    "\n",
    "from scg_detection_tools.models import YOLO_NAS, YOLOv8\n",
    "from scg_detection_tools.detect import Detector\n",
    "from scg_detection_tools.utils.file_handling import read_yaml\n",
    "\n",
    "cfg = read_yaml(\"analyze_config.yaml\")\n",
    "# model = YOLO_NAS(model_arch=cfg[\"yolonas_arch\"], \n",
    "#                 checkpoint_path=cfg[\"yolonas_model_path\"], \n",
    "#                 classes=cfg[\"data_classes\"])\n",
    "model = YOLOv8(yolov8_ckpt_path=cfg[\"yolov8_model_path\"])\n",
    "det_params = cfg[\"detect_parameters\"]\n",
    "det_params[\"embed_slice_callback\"] = None\n",
    "det = Detector(detection_model=model, detection_params=det_params)\n",
    "\n",
    "\n",
    "from analyze import analyze_classify\n",
    "\n",
    "imgs = [\n",
    "    \"/home/juliocesar/leaf-detection/scg-leaf-count/imgs/test/5_640x640_aglom.png\",\n",
    "    \"/home/juliocesar/leaf-detection/scg-leaf-count/imgs/test/0_640x640.png\",\n",
    "]\n",
    "\n",
    "class_color = {\n",
    "    \"light\": \"deepskyblue\",\n",
    "    \"medium\": \"lime\",\n",
    "    \"dark\": \"red\",\n",
    "    \"dead\": \"black\",\n",
    "}\n",
    "\n",
    "analyze_classify(detector=det,\n",
    "                 imgs=imgs,\n",
    "                 cls_labels=[\"dark\", \"dead\", \"light\", \"medium\"],\n",
    "                 cls_colors=class_color,\n",
    "                 method=\"svm\",\n",
    "                 sam2_ckpt_path=cfg[\"sam2_ckpt_path\"],\n",
    "                 sam2_cfg=cfg[\"sam2_cfg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f269d-4b5a-4ee7-a059-1e375344c8df",
   "metadata": {},
   "source": [
    "# Pixel density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c832d-3d6e-4d72-a9c4-9eec91730610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa0ce2-ddfd-49c4-81cd-5e3d6cd0d682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from analyze import analyze_pixel_density\n",
    "from scg_detection_tools.utils.file_handling import get_all_files_from_paths\n",
    "from pathlib import Path\n",
    "\n",
    "IMG_DIR = \"/home/juliocesar/leaf-detection/scg-leaf-count/imgs/analysis\"\n",
    "#LBL_DIR = \"/home/juliocesar/leaf-detection/save/first_analysis_detections/out_cache/\"\n",
    "\n",
    "imgs = get_all_files_from_paths(IMG_DIR)\n",
    "\n",
    "def sort_stem(item):\n",
    "    s = Path(item).stem\n",
    "    try:\n",
    "        val = int(s)\n",
    "        return val\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "imgs = sorted(imgs, key=sort_stem)\n",
    "print(imgs)\n",
    "\n",
    "densities = analyze_pixel_density(model=model, \n",
    "                                  detector=det, \n",
    "                                  imgs=imgs, \n",
    "                                  sam2_ckpt_path=cfg[\"sam2_ckpt_path\"],\n",
    "                                  sam2_cfg=cfg[\"sam2_cfg\"],\n",
    "                                  boxes=False,\n",
    "                                  segments=True,\n",
    "                                  slice_detection=True,\n",
    "                                  on_slice=True,\n",
    "                                  seg_annotations=None,\n",
    "                                  cached_detections=None,\n",
    "                                  show=True,\n",
    "                                  save=False)\n",
    "print(densities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
